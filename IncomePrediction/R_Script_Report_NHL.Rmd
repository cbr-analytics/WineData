---
title: "Linear Regression NHL prediction_kaggle"
author: "Chandra Bhushan Roy"
date: "April 14, 2018"
output: html_document
---

<!-- ``{r setup, include=FALSE} -->
<!-- #knitr::opts_chunk$set(echo = TRUE) -->
<!-- -#``` -->
<style>
pre code, pre, code {
  white-space: pre !important;
  overflow-x: scroll !important;
  word-break: keep-all !important;
  word-wrap: initial !important;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(width=200)
```


## R Markdown

This is a R Markdown document to predict the salary of National Hockey League players. For more details on using R Markdown see <http://rmarkdown.rstudio.com>. This code islinear regression model implementation on NHL data set available on kaggle. I am thank full to kernel avilable at https://www.kaggle.com/camnugent/nhl-salary-data-prediction-cleaning-and-modeling prepared by Cam Nuget.   


```{r eval=FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
#install.packages("plyr")
#install.packages("stringr")
#install.packages("magrittr")
#install.packages("scatterplot3d")

library(tidyverse) 
library(plyr)
library(magrittr)
library(stringr)

library(scatterplot3d)
```
## Set working directory in R


```{r, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
setwd("D:/2018/Upwork/JB125")

```

## Load the NHL data
Loading the data requires calling suitable R functions. The function depends on the type of file. For example, our dataset is in comma separated foramt i.e. csv; therefore, we can use R function "read.csv()"" as demonstrated below. 



```{r , tidy=TRUE, tidy.opts=list(width.cutoff=60)}
train.df <- read.csv("./train.csv", header = TRUE)
colnames(train.df)  # Check column names
head(train.df, 5)
```


```{r , tidy=TRUE, tidy.opts=list(width.cutoff=60)}
test.df <- read.csv("./test.csv", header = TRUE)
colnames(test.df)     # Check column names
head(test.df, 5)
```

## Data Cleaning


Before cleaning data let us combine the two set of data  as most of the features are same, with exception of "Salary" in the train dataset. "Salary" is the predictor variable (dependent). We will make a model to predict salary of players.  

Let us check number of columns in each dataset. 
```{r , tidy=TRUE, tidy.opts=list(width.cutoff=60)}
dim.data.frame(train.df)
dim.data.frame(test.df)

```
Combine train and test datasets.
```{r , tidy=TRUE, tidy.opts=list(width.cutoff=60)}

#test_x$TrainTest <- "test"
#train_y$TrainTest <- "train"

train_data <- train.df[,-c(1)]
dim(train_data)
test_data <- test.df
dim(test_data)

#test <-cbind(test_y, test_x)
all_data <- rbind(train_data, test_data)

#######
colnames(all_data)
dim(all_data)

```

Check the columns with missing values. 

```{r , tidy=TRUE, tidy.opts=list(width.cutoff=60)}

all_missing_list = colnames(all_data)[colSums(is.na(all_data)) > 0]
print(all_missing_list)

```

## Imputation


### State
The column Pr.St states which American or Canadian province the player belongs to. I will imput INT for rest of the players. 

```{r , tidy=TRUE, tidy.opts=list(width.cutoff=60)}
library(plyr)
#fill the Pr.St column with 'INT' for international players
all_data$Pr.St = mapvalues(all_data$Pr.St, from = "", to="INT")

```
### Team
Team column states which team a player plyed for. 
Some players have multiple teams they played for. We will split each time into its own boolean predictor and those who player for multiple teams are recorded accordingly.

```{r eval = TRUE, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
#Make team boolean columns
#get the unique list of team acronymns
teams = c()    # A list
for( i in levels(all_data$Team)){
  x = strsplit(i, "/") # Split the string and store the values as list in "x"
  #print(x)
  for(y in x){
    teams = c(teams, y) # Combine all the values of x in a list "teams"
  #  print(y)
  #  print(teams)
    
  }
}
teams = unique(teams)  # assign unique entires to list teams 
print(teams)

# add columns with the team names as the header and 0 as values
for(j in teams){
  all_data[,j] = 0 # Assign inital values 0 to each new column created in the loop
  print(j)
  
}
head(all_data,5) # Check the new columns created. 
print(all_data$team)
#iterate through and record the teams for each player
for(i in 1:length(all_data$Team)){
  teams_of_person = strsplit(as.character(all_data$Team[i]), "/")[[1]]
  print(teams_of_person)
  for(x in teams_of_person){
    all_data[,x][i] = 1	     # Assign value 1 for each column_team with which player is associated
    #print("hello")
  }
}
print(head(all_data))
```
### Positions played
Splitting of the categorical position column into multiple booleans. Most players have a single position such as "D" (defence) for those with multiple positions, we will record this fact.

```{r , tidy=TRUE, tidy.opts=list(width.cutoff=60)}
#Make position boolean columns
pos = c()
for( i in levels(all_data$Position)){
	x = strsplit(i, "/")
	for(y in x){
		pos = c(pos, y)
	}
}
pos = unique(pos)
print(pos)

# add columns with the pos names as the header and 0 as values
for(position in pos){
	all_data[,position] = 0
}

#iterate through and record the position(s) for each player
for(i in 1:length(all_data$Position)){
	pos_of_person = strsplit(as.character(all_data$Position[i]), "/")[[1]]
	for(x in pos_of_person){
		all_data[,x][i] = 1	
	}
}
print(head(all_data))
```
### Year, day and month of birth
We need to manipulate the date of birth into three columns: year of birth, month of birth, and day of birth. There is special care givent to the year column to account for players born after the year 2000. Some of you may find it odd that I am giving day and month their own predictors. It is an often quoted fact in Canada that the month of someone's birth affects their chances of making the NHL, the logic being that if you are born early in the year then as a child you are several months older than some of the other kids in your age class and this gives you a development advantage. I don't know if this is true but we will include the data to check if it is contributing to the final random forest model!

```{r , tidy=TRUE, tidy.opts=list(width.cutoff=60)}
#turn the born column into 
# an age column 
# 3 integer columns year:month:date
library(stringr)


# Objective: standardize year, month and day and create separate columns for each.
bday_parts = str_split_fixed(all_data$Born, "-", 3)

#adjust year column to account for missing digits
birth_year = c()  # A list created for storing players' year of birth
for(year in bday_parts[,1]){   # Read from the first columns of bday_parts
  if(as.numeric(year) < 10){      ## It is two digit year, so payers born in 21st centry must be younger
    yr = paste("20", year, sep="") # Players born in 21st century 
    birth_year = c(birth_year, yr)   # Store the new values in birth_year
  }else{
    yr = paste("19",year, sep="")  # If player are not born in 21st century append 19 before the year. 
    birth_year = c(birth_year, yr)
  }
}

all_data$birth_year <- as.numeric(birth_year)       # Create separate column for YEAR & add to all_data
all_data$birth_month <- as.numeric(bday_parts[,2])  # Create separate column for MONTH & add to all_data
all_data$birth_day <- as.numeric(bday_parts[,3])   # Create separate column for DAY & add to all_data
head(all_data)

```

### Country and Nationality of players

```{r , tidy=TRUE, tidy.opts=list(width.cutoff=60)}
# split Cntry and Nat to boolean columns

birth_country = levels(all_data$Cntry)
# add columns with the country of birth options
# note the Estonia for Uncle Leo
for(country in birth_country){
	c = paste("born", country, sep="_")

	all_data[,c] = 0
}

#iterate through and record the birth country of each player
for(i in 1:length(all_data$Cntry)){
	birth_country = all_data$Cntry[i]
	c = paste("born", birth_country, sep="_")
	all_data[,c][i] = 1	
}


nationality = levels(all_data$Nat)
for(country in nationality){
	c = paste("nation", country, sep="_")
	all_data[,c] = 0
}

#iterate through and record the birth country of each player
for(i in 1:length(all_data$Nat)){
	nationality = all_data$Nat[i]
	c = paste("nation", nationality, sep="_")
	all_data[,c][i] = 1	
}

head(all_data)
```
## Numerical columns imputation


```{r , tidy=TRUE, tidy.opts=list(width.cutoff=60)}
all_data$undrafted = is.na(all_data$DftRd)

#fill median values
#loop through the dataframe, filling each column with the median of 
#the existing values for the entire dataset
#where are there still missing values?

all_missing_list =  colnames(all_data)[colSums(is.na(all_data)) > 0]

length(all_missing_list) == 0    # Flag to check NA values
#if above true all values are imputed!

for( i in 1:length(all_missing_list)){
	#get the global median
	median_all <- median(all_data[,all_missing_list[i]], na.rm =TRUE) # Neglect NA when calculating #+ median
	print(median_all)
	#imput the missing values with the column's median
	all_data[,all_missing_list[i]][is.na(all_data[,all_missing_list[i]])] <- median_all
}

all_missing_list <- colnames(all_data)[colSums(is.na(all_data))]

length(all_missing_list) == 0   # Flag to check NA values
```

# EDA (Exploratory Data Analysis)

How many players from each country?
```{r , tidy=TRUE, tidy.opts=list(width.cutoff=60)}

barplot(sort(table(all_data$Nat ),
            decreasing = TRUE),
       horiz = TRUE,
       las=1,
       col = c("red", "blue4", "blue", "red3", "skyblue"),
       main = "Number of NHL players from each country",
       ylab = "Country", xlab= "count")
```

# Age Breakdown

```{r , tidy=TRUE, tidy.opts=list(width.cutoff=60)}
table(all_data$birth_year)
```

## Age Histogram 
```{r , tidy=TRUE, tidy.opts=list(width.cutoff=60)}
hist(all_data$birth_year, breaks=28, 
     col="skyblue", xlab='Year of birth', 
     main='Distribution of NHL players by birth year (2016/2017 season)\nA.K.A. Jaromir Jagr the ageless one')
```

# Salary Distribution
```{r , tidy=TRUE, tidy.opts=list(width.cutoff=60)}
summary(train.df$Salary)
hist(train.df$Salary, breaks=52,col="salmon", xlab='Salary', 
     ylab = "Number of players", main='NHL Salary Distribution: 2016/2017')
```

## Compare Salary with Ice-Performance (No Goals)
```{r , tidy=TRUE, tidy.opts=list(width.cutoff=60)}
plot(train.df$G, train.df$Salary, xlab = "No. Goals", pch =20, ylab = 'Money Earned')

abline(lm(train.df$Salary ~ train.df$G), col = 'blue')
```


```{r , tidy=TRUE, tidy.opts=list(width.cutoff=60)}
plot(train.df$G, train.df$Salary, pch=20, xlab='goals scored', ylab='money earned', main="Who are the outliers?")
abline(lm(train.df$Salary ~ train.df$G), col="red")
text(train.df$G, train.df$Salary, labels=train.df$Last.Name, cex=0.7, pos = 3)
```

# Final train and test dataset
```{r , tidy=TRUE, tidy.opts=list(width.cutoff=60)}
#train.df2$Salary <- train.df$Salary
#train.df3 <- cbind(train.df[,1], all_data[1:612,]) 
train.df2 <- all_data[c(1:612),]


train.final <- all_data[c(1:612),]
train.final$Salary <- train.df$Salary

test.final <- all_data[c(613:874),]


```
#  Effects of goals and age on salary
Before ploting 3D plot, let us preapre a new dataset from the all_data to have a clean train data set. 

```{r , tidy=TRUE, tidy.opts=list(width.cutoff=60)}
library(scatterplot3d)
#To allow a colour gradient on out graphs

#plot
sd3 = scatterplot3d(train.df2$G, train.df2$birth_year, train.df2$Salary, 
                    pch =19,
                    type = 'h',
                    main="Interaction of age, goals and salary",
                    zlab="Salary",
                    xlab="Goals",
                    ylab="Birth Year")
                    #grid=TRUE
                    
```

## Examining multiple on ice metrics and salary

```{r , tidy=TRUE, tidy.opts=list(width.cutoff=60)}
score_3d = scatterplot3d(train.df2$TOI,train.df2$SCF,train.df2$Salary,pch=19,
                         type="h",
                         cex.axis=0.5,
                         las=1,
                         lty.hplot=2,
                         #color=color.gradient(all_data$Salary, 
                         #colors=c("black","skyblue")),
                         main="More relationships:\nSalary, time on ice, and scoring chances",
                         zlab="Salary",xlab="Time on ice(s)",
                         ylab="scoring chances while player on ice",
                         grid=TRUE)
```




```{r , tidy=TRUE, tidy.opts=list(width.cutoff=60)}
score_3d = scatterplot3d(train.df2$TOI,train.df2$SCF,train.df2$Salary,pch=19,
                         type="h",
                         cex.axis=0.5,
                         las=1,
                         lty.hplot=2,
                         #color=color.gradient(all_data$Salary, 
                         #colors=c("black","skyblue")),
                         main="More relationships:\nSalary, time on ice, and scoring chances",
                         zlab="Salary",xlab="Time on ice(s)",
                         ylab="scoring chances while player on ice",
                         grid=TRUE)
#score_3d.coords = score_3d$xyz.convert(all_data$TOI,all_data$SCF,all_data$Salary)
#text(score_3d.coords$x, score_3d.coords$y,labels=all_data$Last.Name,cex=.5, pos=4)
```

## Correlation

```{r , tidy=TRUE, tidy.opts=list(width.cutoff=60)}
pairs(~ Born+ +  Ht + Wt +DftRd + 
        Position+Team+GP+G , data = train.df2,  main="Simple Scatterplot Matrix")

str(train.final)
str(test.final)
```

```{r , tidy=TRUE, tidy.opts=list(width.cutoff=60)}
colnames(train.final)
```

 


```{r , tidy=TRUE, tidy.opts=list(width.cutoff=60)}
cor(train.final$Salary, train.final$birth_year)

```

## Removing Outliers
First determine the Interquartile range(IQR) for the feature for which you want to remove outlier. 
Here let us remove outlier from the birth year and replace the outlier point with maximum. 

First do the boxplot to chec kthe outliers. 


```{r , tidy=TRUE, tidy.opts=list(width.cutoff=60)}
boxplot( train.final$birth_year, data = train.final)

```

You can note couple of points at the lower end. These outliers can be removed or replaced with suitable value. Here we will replace the outlierw with suitable value (Q1)
Q1: Value of 1st quartile
Q3: Value of 3rd quartile

new_value_birth_year = Q1 - 1.5×IQR

If the outliers were at top end then,
new_value_birth_year = Q3 + 1.5 IQR.

Let us find the values of IQR, Q1, Q3, and new_value_birth_year

```{r , tidy=TRUE, tidy.opts=list(width.cutoff=60)}
summary(train.final$birth_year)
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    1972    1987    1991    1990    1993    1998
# From above we have 
Q1 <- 1987
Q3 <- 1993
IQR <- Q3-Q1

new_value_birth_year <- Q1 - (1.5 * IQR)
new_value_birth_year

```

Now replace the outliers with new_value 

```{r , tidy=TRUE, tidy.opts=list(width.cutoff=60)}
for(i in 1:nrow(train.final)){
  if(train.final$birth_year[i] < new_value_birth_year )
    train.final$birth_year[i] <- new_value_birth_year
}
```
Again plot hte boxplot to see if the outliers exist. Ideally the outliers should be gone. 

```{r , tidy=TRUE, tidy.opts=list(width.cutoff=60)}
boxplot( train.final$birth_year, data = train.final)

```


## Linear Regression (1st Approch)

### STEPS to model and predict dependent Variable 
 *3.1 Split the training data into train and test data for validations
 
 *3.2 Run the Linear gression model
 
 *3.3 Predict uisng new model.
 
 *3.4 Check accuracy.  
 
### Train and test data
```{r , tidy=TRUE, tidy.opts=list(width.cutoff=60)}
train.final <- train.final[,-c(1,2,12,13,14,15)]
test.final <- test.final[,-c(1,2,12,13,14,15)]

train.final <- train.final[1:612,]
test.final <- train.final[613:874,]
dim(train.final)
dim(test.final)


```
### Normalization

* Data normalization can be achieved using Z-transform. Please note, there are many ways to normalize a given numerical data, z-transform being onely one among them. 

#### Normalized value of Height
```{r , tidy=TRUE, tidy.opts=list(width.cutoff=60)}
# Normalize Height (Ht)

mean_ht <- mean(train.final$Ht) # Store mean value of the columns
std_ht <- sd(train.final$Ht) # Store the standard deviation of the columns
mean_ht
std_ht
for(i in 1:nrow(train.final)){        # A for loop to compute the normalized value of each row element of given column
 train.final$Ht_n[i] <-(train.final$Ht[i] - mean(train.final$Ht)) / sd(train.final$Ht)
}
head(train.final$Ht_n)

#mean(train.final$Ht_n)

```
#### Normalized value of Weight
```{r , tidy=TRUE, tidy.opts=list(width.cutoff=60)}
# Normalize Weight (Wt)

mean_wt <- mean(train.final$Wt) # Store mean value of the columns
std_wt <- sd(train.final$Wt) # Store the standard deviation of the columns
mean_wt
std_wt
for(i in 1:nrow(train.final)){        # A for loop to compute the normalized value of each row element of given column
 train.final$Wt_n[i] <-(train.final$Wt[i] - mean(train.final$Wt)) / sd(train.final$Wt)
}
head(train.final$Wt_n)

#mean(train.final$Ht_n)

```
#### Normalize the valued of the birth_year

```{r , tidy=TRUE, tidy.opts=list(width.cutoff=60)}
# Normalize Weight (Wt)
mean_birth_year <- mean(train.final$birth_year) # Store mean value of the columns
std_birth_year <- sd(train.final$birth_year) # Store the standard deviation of the columns
mean_birth_year
std_birth_year
for(i in 1:nrow(train.final)){        # A for loop to compute the normalized value of each row element of given column
 train.final$birth_year_n[i] <-(train.final$birth_year[i] - mean(train.final$birth_year)) / sd(train.final$birth_year)
}
head(train.final$birth_year_n)

#mean(train.final$Ht_n)

```
### Model  
Based on the initial EDA, we have identified 4 independ variables for this linear regression model namely Height, Weight, Ovrl, Goal(G), and Birth Year. We are passing the normalized values of the same to the linear model. 
```{r , tidy=TRUE, tidy.opts=list(width.cutoff=60)}
lm_model <- lm(Salary ~Ht_n+Wt_n+Ovrl+G+birth_year_n, data = train.final)
lm_model
summary(lm_model)
```


### Predict

```{r , tidy=TRUE, tidy.opts=list(width.cutoff=60)}
predicted_salary <- predict(lm_model, train.final)
#predicted_salary
```



### Check Accuracy 
```{r , tidy=TRUE, tidy.opts=list(width.cutoff=60)}
actual_preds <- data.frame(cbind(actuals = train.final$Salary, predicteds = predicted_salary))
corrleation_accuracy <- cor(actual_preds)
corrleation_accuracy

head(actual_preds)
```

## Linear Regression (2nd Approach)

### Train and test data (80:20 ratio)
```{r , tidy=TRUE, tidy.opts=list(width.cutoff=60)}
#train.final <- train.final[,-c(1,2,12,13,14,15)]
#test.final <- test.final[,-c(1,2,12,13,14,15)]

data = train.final  # Store the train data before spliting

split_indexes <- sample(1:nrow(data), size = 0.2*nrow(data))

test <- data[split_indexes,]
train <- data[-split_indexes,]

dim(train)
dim(test)


```

### Model

```{r , tidy=TRUE, tidy.opts=list(width.cutoff=60)}
lm_model <- lm(Salary ~Ht_n+Wt_n+Ovrl+G+birth_year_n, data = train)
lm_model
summary(lm_model)
```


### Predict


```{r , tidy=TRUE, tidy.opts=list(width.cutoff=60)}
predicted_salary <- predict(lm_model, test)

```

### Check Accuracy

```{r , tidy=TRUE, tidy.opts=list(width.cutoff=60)}
actual_preds <- data.frame(cbind(actuals = test$Salary, predicteds = predicted_salary))
corrleation_accuracy <- cor(actual_preds)
corrleation_accuracy

head(actual_preds)
```



```{r , tidy=TRUE, tidy.opts=list(width.cutoff=60)}

```




```{r , tidy=TRUE, tidy.opts=list(width.cutoff=60)}

```
Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
